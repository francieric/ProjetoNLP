{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXb03qIuHyo0",
    "outputId": "97e968c8-5dd1-44ce-e64e-16a130d21af3"
   },
   "source": [
    "### Projeto Classificador de Twitter\n",
    "#### RESUMO\n",
    "\n",
    "<p align='justify'> Este Projeto teve como objetivo avaliar o impacto dos métodos de tratamento nos tweets para retirada de stopwords, tokenizacao, correção de palavras feitos de forma manual em comparação da adoção da biblioteca spacy(https://spacy.io/) para tratamento automático e tokenização. \n",
    "    \n",
    "    Utilizando uma técnica chamada Word2Vec, cuja ideia é transformar cada palavra (a.k.a. token) do nosso conjunto de frases (a.k.a. corpus) em um vetor numérico que a represente semanticamente, podemos obter representações com propriedades bem interessantes. Um exemplo é que em um contexto o vetor que representa o token “Madri”, subtraído do vetor representante de “Espanha” e somado ao vetor de “França” será muito próximo do vetor obtido para “Paris”. Ou de uma forma equacionada:\n",
    "vec(“Madri”) — vec(“Espanha”) + vec(“França”) ≈ vec(“Paris”)\n",
    "Este modelo também é capaz de capturar relações entre as palavras como, por exemplo: “homem” está para “mulher” assim como “rei” está para “rainha”.\n",
    "    \n",
    "    O Word2Vec pertence a uma classe de modelos conhecidos na literatura como neural language models, pois utiliza redes neurais para aprender as suas representações. Existem dois algoritmos que são utilizados para o seu treinamento: Continuous bag of words (a.k.a. CBOW) e Skip-gram. \n",
    "    \n",
    "    - CBOW: A ideia do algoritmo Continuous bag of words é: prever qual a palavra que estamos buscando a partir de um determinado contexto. \n",
    "    - Skip-gram: Já a abordagem do Skip-gram é a inversa: tomando como ponto de partida uma determinada palavra, o objetivo é prever o contexto do qual esta palavra veio. FONTE: https://medium.com/luizalabs/similaridade-entre-t%C3%ADtulos-de-produtos-com-word2vec-5e26199862f0#:~:text=CBOW%3A%20A%20ideia%20do%20algoritmo,partir%20de%20um%20determinado%20contexto.&text=Skip%2Dgram%3A%20J%C3%A1%20a%20abordagem,do%20qual%20esta%20palavra%20veio.\n",
    "    \n",
    "    Utizando como métrica de comparação os resultados obtidos com o modelo de regressão logistica, aplicados com a seguinte divisão:\n",
    "    - CBOW com tratamento automático\n",
    "    - CBOW com tratamento manual\n",
    "    - SKIPGRAM com tratamento automático\n",
    "    - SKIPGRAM com tratamento manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9TWrwS0Es6m",
    "outputId": "fafea11e-346a-4b2e-f9c6-e5bafba29b27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/eric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import wordninja\n",
    "import textblob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "QIjpRYkPA825",
    "outputId": "092a4406-9d33-4957-da8a-ddd36576b602"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Dataset\n",
    "dados = pd.read_csv('/Users/eric/Downloads/labeled_data.csv')\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "UdeFDaqcEq7b",
    "outputId": "da5ece5f-9f1c-49ee-afb6-0c2d27d3cd54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[Neutro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet                classe  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...              [Neutro]  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  [Linguagem ofensiva]  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  [Linguagem ofensiva]  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  [Linguagem ofensiva]  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  [Linguagem ofensiva]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Criação de dicionário para classificação\n",
    "Classe = {0: ['Discurso de ódio'], 1: ['Linguagem ofensiva'], 2: ['Neutro']}\n",
    "dados['classe']= dados['class'].map(Classe)\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Tbi17BIhE7Kg"
   },
   "outputs": [],
   "source": [
    "# Excluindo da descrição os números, informações julgadas irrelevantes para a classificação.\n",
    "dados['tweet_novo'] = dados['tweet'].str.replace('[0-9]+', '', regex=True).copy()\n",
    "# Excluindo da descrição puntuação, informações julgadas irrelevantes para a classificação.\n",
    "dados['tweet_novo'] = dados['tweet_novo'].str.replace('[,.:;!?]+', ' ', regex=True).copy()\n",
    "# Excluindo da descrição caracteres especiais, informações julgadas irrelevantes para a classificação.\n",
    "dados['tweet_novo'] = dados['tweet_novo'].str.replace(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", regex=True).copy()\n",
    "\n",
    "# Colocando todos os caracteres em caixa baixa.\n",
    "dados['tweet_novo'] = dados['tweet_novo'].str.lower().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "Rfs9zwJfE5Hp",
    "outputId": "98558f14-c770-4ebc-f56f-6b6c8a962647"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "      <th>tweet_novo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[Neutro]</td>\n",
       "      <td>rt    as a woman you shouldn't complain abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    boy dats cold tyga dwn bad for cuffin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt   dawg  rt    you ever fuck a bitch and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt  _g_anderson   _based she look like a tranny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    the shit you hear about me might be tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet                classe  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...              [Neutro]   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  [Linguagem ofensiva]   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  [Linguagem ofensiva]   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  [Linguagem ofensiva]   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  [Linguagem ofensiva]   \n",
       "\n",
       "                                          tweet_novo  \n",
       "0    rt    as a woman you shouldn't complain abou...  \n",
       "1    rt    boy dats cold tyga dwn bad for cuffin ...  \n",
       "2    rt   dawg  rt    you ever fuck a bitch and s...  \n",
       "3    rt  _g_anderson   _based she look like a tranny  \n",
       "4    rt    the shit you hear about me might be tr...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzCY9_uUFU1L",
    "outputId": "5fbb34f1-e856-4e0a-9677-e3b67e7e74ee"
   },
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "gb7QgGSuHlnw",
    "outputId": "5120aefa-c930-4828-cadc-73a7296d839f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "      <th>tweet_novo</th>\n",
       "      <th>tweet_organizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[Neutro]</td>\n",
       "      <td>rt    as a woman you shouldn't complain abou...</td>\n",
       "      <td>womann't complain cleaning house amp man alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    boy dats cold tyga dwn bad for cuffin ...</td>\n",
       "      <td>boy dats cold tyga dwn bad cuffin dat hoe st p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet                classe  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...              [Neutro]   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  [Linguagem ofensiva]   \n",
       "\n",
       "                                          tweet_novo  \\\n",
       "0    rt    as a woman you shouldn't complain abou...   \n",
       "1    rt    boy dats cold tyga dwn bad for cuffin ...   \n",
       "\n",
       "                                    tweet_organizado  \n",
       "0  womann't complain cleaning house amp man alway...  \n",
       "1  boy dats cold tyga dwn bad cuffin dat hoe st p...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para retirar stop words\n",
    "punct = list(string.punctuation)\n",
    "stop_words = stopwords.words('english')\n",
    "additional_stop_words = ['RT', 'rt', 'via', '...', 'http', 'twitpic',\n",
    "'tinyurl' ,'www']\n",
    "stopword_list = punct + stop_words + additional_stop_words\n",
    "def tokenize_df(tokenized_words):\n",
    "  tokenized_words = word_tokenize(tokenized_words)\n",
    "  stop = [word for word in tokenized_words if word not in stopword_list]\n",
    "  text = TreebankWordDetokenizer().detokenize(stop)\n",
    "  return text\n",
    "# Eliminando as stop words\n",
    "dados['tweet_organizado'] = dados['tweet_novo'].apply(tokenize_df).copy()\n",
    "dados.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "8KnB_NqIIsHq",
    "outputId": "61e58118-2851-4dc6-8bd0-ef394a3a8688"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "      <th>tweet_novo</th>\n",
       "      <th>tweet_organizado</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_novo2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[Neutro]</td>\n",
       "      <td>rt    as a woman you shouldn't complain abou...</td>\n",
       "      <td>womann't complain cleaning house amp man alway...</td>\n",
       "      <td>[woman, n, ', t, complain, cleaning, house, am...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    boy dats cold tyga dwn bad for cuffin ...</td>\n",
       "      <td>boy dats cold tyga dwn bad cuffin dat hoe st p...</td>\n",
       "      <td>[boy, dat, s, cold, tyga, dw, n, bad, cuff, in...</td>\n",
       "      <td>boy dat s cold tyga dw n bad cuff in dat hoe s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt   dawg  rt    you ever fuck a bitch and s...</td>\n",
       "      <td>dawg ever fuck bitch start cry confused shit</td>\n",
       "      <td>[daw, g, ever, fuck, bitch, start, cry, confus...</td>\n",
       "      <td>daw g ever fuck bitch start cry confused shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt  _g_anderson   _based she look like a tranny</td>\n",
       "      <td>_g_anderson _based look like tranny</td>\n",
       "      <td>[g, anderson, based, look, like, tr, an, ny]</td>\n",
       "      <td>g anderson based look like tr an ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    the shit you hear about me might be tr...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "      <td>[shit, hear, might, true, might, faker, bitch,...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet                classe  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...              [Neutro]   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  [Linguagem ofensiva]   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  [Linguagem ofensiva]   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  [Linguagem ofensiva]   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  [Linguagem ofensiva]   \n",
       "\n",
       "                                          tweet_novo  \\\n",
       "0    rt    as a woman you shouldn't complain abou...   \n",
       "1    rt    boy dats cold tyga dwn bad for cuffin ...   \n",
       "2    rt   dawg  rt    you ever fuck a bitch and s...   \n",
       "3    rt  _g_anderson   _based she look like a tranny   \n",
       "4    rt    the shit you hear about me might be tr...   \n",
       "\n",
       "                                    tweet_organizado  \\\n",
       "0  womann't complain cleaning house amp man alway...   \n",
       "1  boy dats cold tyga dwn bad cuffin dat hoe st p...   \n",
       "2       dawg ever fuck bitch start cry confused shit   \n",
       "3                _g_anderson _based look like tranny   \n",
       "4     shit hear might true might faker bitch told ya   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [woman, n, ', t, complain, cleaning, house, am...   \n",
       "1  [boy, dat, s, cold, tyga, dw, n, bad, cuff, in...   \n",
       "2  [daw, g, ever, fuck, bitch, start, cry, confus...   \n",
       "3       [g, anderson, based, look, like, tr, an, ny]   \n",
       "4  [shit, hear, might, true, might, faker, bitch,...   \n",
       "\n",
       "                                          text_novo2  \n",
       "0  woman n' t complain cleaning house amp man alw...  \n",
       "1  boy dat s cold tyga dw n bad cuff in dat hoe s...  \n",
       "2      daw g ever fuck bitch start cry confused shit  \n",
       "3                g anderson based look like tr an ny  \n",
       "4     shit hear might true might faker bitch told ya  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separando palavras juntas\n",
    "dados['text_split'] = dados['tweet_organizado'].apply(wordninja.split)\n",
    "dados['text_novo2'] = dados['text_split'].apply(TreebankWordDetokenizer().detokenize)\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTIPY4sBMDlb",
    "outputId": "a4da01ce-5c8c-4861-fdea-31f4c501fc3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.167536314328512 minutos\n"
     ]
    }
   ],
   "source": [
    "#corrigindo palavras incorretas\n",
    "from time import time\n",
    "t0=time()\n",
    "dados['tweet_definitivo'] = dados['text_novo2'].apply(textblob.TextBlob).apply(textblob.TextBlob.correct).apply(str)\n",
    "dados.head(2)\n",
    "tf=(time()-t0)/60\n",
    "print(f'{tf} minutos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Vw6IbQ6tFxEI"
   },
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "yllK-MTMV_wb",
    "outputId": "67be7c7c-0de4-4363-ec11-2889e4ddde74"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "      <th>tweet_novo</th>\n",
       "      <th>tweet_organizado</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_novo2</th>\n",
       "      <th>tweet_definitivo</th>\n",
       "      <th>eric_tweet_definito</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>['Neutro']</td>\n",
       "      <td>rt    as a woman you shouldn't complain abou...</td>\n",
       "      <td>womann't complain cleaning house amp man alway...</td>\n",
       "      <td>['woman', 'n', \"'\", 't', 'complain', 'cleaning...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>rt woman complain cleaning house amp man trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>['Linguagem ofensiva']</td>\n",
       "      <td>rt    boy dats cold tyga dwn bad for cuffin ...</td>\n",
       "      <td>boy dats cold tyga dwn bad cuffin dat hoe st p...</td>\n",
       "      <td>['boy', 'dat', 's', 'cold', 'tyga', 'dw', 'n',...</td>\n",
       "      <td>boy dat s cold tyga dw n bad cuff in dat hoe s...</td>\n",
       "      <td>boy dat s cold tea do n bad cuff in dat he st ...</td>\n",
       "      <td>rt boy dats cold tyga dwn bad cuffin dat hoe p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>['Linguagem ofensiva']</td>\n",
       "      <td>rt   dawg  rt    you ever fuck a bitch and s...</td>\n",
       "      <td>dawg ever fuck bitch start cry confused shit</td>\n",
       "      <td>['daw', 'g', 'ever', 'fuck', 'bitch', 'start',...</td>\n",
       "      <td>daw g ever fuck bitch start cry confused shit</td>\n",
       "      <td>day g ever fuck bitch start cry confused shit</td>\n",
       "      <td>rt dawg rt fuck bitch start cry confused shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>['Linguagem ofensiva']</td>\n",
       "      <td>rt  _g_anderson   _based she look like a tranny</td>\n",
       "      <td>_g_anderson _based look like tranny</td>\n",
       "      <td>['g', 'anderson', 'based', 'look', 'like', 'tr...</td>\n",
       "      <td>g anderson based look like tr an ny</td>\n",
       "      <td>g anderson based look like tr an ny</td>\n",
       "      <td>rt look like tranny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>['Linguagem ofensiva']</td>\n",
       "      <td>rt    the shit you hear about me might be tr...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "      <td>['shit', 'hear', 'might', 'true', 'might', 'fa...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "      <td>shit hear might true might baker bitch told a</td>\n",
       "      <td>rt shit hear true faker bitch told ya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                eric_tweet_definito\n",
       "0           0  ...     rt woman complain cleaning house amp man trash\n",
       "1           1  ...  rt boy dats cold tyga dwn bad cuffin dat hoe p...\n",
       "2           2  ...      rt dawg rt fuck bitch start cry confused shit\n",
       "3           3  ...                                rt look like tranny\n",
       "4           4  ...              rt shit hear true faker bitch told ya\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dados.to_csv('/content/drive/MyDrive/Projeto Daniel/procesado_labeled_data.csv')\n",
    "##Dataset\n",
    "dados = pd.read_csv('/content/drive/MyDrive/Projeto Daniel/procesado_labeled_data.csv')\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ulxM1Y8Gewh",
    "outputId": "0aa32b73-1aa1-44e6-f934-c9b9f7e68620"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7fab30029950>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_para_tratamento_eric = (titulo.lower() for titulo in dados['tweet'])\n",
    "texto_para_tratamento_eric\n",
    "\n",
    "textos_tratados_helano = (titulo for titulo in dados['tweet_definitivo'])\n",
    "textos_tratados_helano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "UhQOJ0xASZFZ"
   },
   "outputs": [],
   "source": [
    "def trata_texto(doc):\n",
    "  tokens_validos=[]\n",
    "  for token in doc:\n",
    "    e_valido= not token.is_stop and token.is_alpha\n",
    "    if e_valido:\n",
    "      tokens_validos.append(token.text)\n",
    "  if len(tokens_validos)>2:\n",
    "      return ' '.join(tokens_validos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQZvRNkbSroX",
    "outputId": "bbd6aa84-cd1f-4dcc-ee73-efe1abd60768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.267153267065684 minutos\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "textos_tratados_eric = [trata_texto(doc) for doc in nlp.pipe(texto_para_tratamento_eric,batch_size=1000,n_process=-1)]\n",
    "\n",
    "tf=(time()-t0)/60\n",
    "print(f'{tf} minutos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 13)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "8yuobmn5S1DU",
    "outputId": "0aae089e-88c6-4847-ea16-d9fec761fdf1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "      <th>tweet_novo</th>\n",
       "      <th>tweet_organizado</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_novo2</th>\n",
       "      <th>tweet_definitivo</th>\n",
       "      <th>eric_tweet_definito</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[Neutro]</td>\n",
       "      <td>rt    as a woman you shouldn't complain abou...</td>\n",
       "      <td>womann't complain cleaning house amp man alway...</td>\n",
       "      <td>[woman, n, ', t, complain, cleaning, house, am...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>rt woman complain cleaning house amp man trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    boy dats cold tyga dwn bad for cuffin ...</td>\n",
       "      <td>boy dats cold tyga dwn bad cuffin dat hoe st p...</td>\n",
       "      <td>[boy, dat, s, cold, tyga, dw, n, bad, cuff, in...</td>\n",
       "      <td>boy dat s cold tyga dw n bad cuff in dat hoe s...</td>\n",
       "      <td>boy dat s cold tea do n bad cuff in dat he st ...</td>\n",
       "      <td>rt boy dats cold tyga dwn bad cuffin dat hoe p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt   dawg  rt    you ever fuck a bitch and s...</td>\n",
       "      <td>dawg ever fuck bitch start cry confused shit</td>\n",
       "      <td>[daw, g, ever, fuck, bitch, start, cry, confus...</td>\n",
       "      <td>daw g ever fuck bitch start cry confused shit</td>\n",
       "      <td>day g ever fuck bitch start cry confused shit</td>\n",
       "      <td>rt dawg rt fuck bitch start cry confused shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt  _g_anderson   _based she look like a tranny</td>\n",
       "      <td>_g_anderson _based look like tranny</td>\n",
       "      <td>[g, anderson, based, look, like, tr, an, ny]</td>\n",
       "      <td>g anderson based look like tr an ny</td>\n",
       "      <td>g anderson based look like tr an ny</td>\n",
       "      <td>rt look like tranny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    the shit you hear about me might be tr...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "      <td>[shit, hear, might, true, might, faker, bitch,...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "      <td>shit hear might true might baker bitch told a</td>\n",
       "      <td>rt shit hear true faker bitch told ya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet                classe  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...              [Neutro]   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  [Linguagem ofensiva]   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  [Linguagem ofensiva]   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  [Linguagem ofensiva]   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  [Linguagem ofensiva]   \n",
       "\n",
       "                                          tweet_novo  \\\n",
       "0    rt    as a woman you shouldn't complain abou...   \n",
       "1    rt    boy dats cold tyga dwn bad for cuffin ...   \n",
       "2    rt   dawg  rt    you ever fuck a bitch and s...   \n",
       "3    rt  _g_anderson   _based she look like a tranny   \n",
       "4    rt    the shit you hear about me might be tr...   \n",
       "\n",
       "                                    tweet_organizado  \\\n",
       "0  womann't complain cleaning house amp man alway...   \n",
       "1  boy dats cold tyga dwn bad cuffin dat hoe st p...   \n",
       "2       dawg ever fuck bitch start cry confused shit   \n",
       "3                _g_anderson _based look like tranny   \n",
       "4     shit hear might true might faker bitch told ya   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [woman, n, ', t, complain, cleaning, house, am...   \n",
       "1  [boy, dat, s, cold, tyga, dw, n, bad, cuff, in...   \n",
       "2  [daw, g, ever, fuck, bitch, start, cry, confus...   \n",
       "3       [g, anderson, based, look, like, tr, an, ny]   \n",
       "4  [shit, hear, might, true, might, faker, bitch,...   \n",
       "\n",
       "                                          text_novo2  \\\n",
       "0  woman n' t complain cleaning house amp man alw...   \n",
       "1  boy dat s cold tyga dw n bad cuff in dat hoe s...   \n",
       "2      daw g ever fuck bitch start cry confused shit   \n",
       "3                g anderson based look like tr an ny   \n",
       "4     shit hear might true might faker bitch told ya   \n",
       "\n",
       "                                    tweet_definitivo  \\\n",
       "0  woman n' t complain cleaning house amp man alw...   \n",
       "1  boy dat s cold tea do n bad cuff in dat he st ...   \n",
       "2      day g ever fuck bitch start cry confused shit   \n",
       "3                g anderson based look like tr an ny   \n",
       "4      shit hear might true might baker bitch told a   \n",
       "\n",
       "                                 eric_tweet_definito  \n",
       "0     rt woman complain cleaning house amp man trash  \n",
       "1  rt boy dats cold tyga dwn bad cuffin dat hoe p...  \n",
       "2      rt dawg rt fuck bitch start cry confused shit  \n",
       "3                                rt look like tranny  \n",
       "4              rt shit hear true faker bitch told ya  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados['eric_tweet_definito']=textos_tratados_eric\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "UMOAadqqijJm"
   },
   "outputs": [],
   "source": [
    "#dados.to_csv('/Users/eric/Downloads/projeto_daniel/29042021procesado_labeled_data_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(24783, 13)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_helano = dados.copy()\n",
    "display(dados_helano.shape)\n",
    "\n",
    "\n",
    "\n",
    "dados_helano = dados_helano.drop('eric_tweet_definito', 1)\n",
    "#display(print(len(dados_helano)))\n",
    "#dados_helano = dados_helano.dropna().drop_duplicates()\n",
    "\n",
    "#print(len(dados_helano))\n",
    "\n",
    "dados_helano.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22831, 13)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## REMOVENDO OS NONETYPE PARA POSSIBILITAR A GERACAO DOS TOKENS\n",
    "\n",
    "dados_eric = dados.copy()\n",
    "display(dados_eric.shape)\n",
    "\n",
    "\n",
    "\n",
    "dados_eric = dados_eric.drop('tweet_definitivo', 1)\n",
    "display(print(len(dados_eric)))\n",
    "\n",
    "dados_eric = dados_eric[dados_eric['eric_tweet_definito'].notna()]\n",
    "\n",
    "print(len(dados_eric))\n",
    "\n",
    "dados_eric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0             22831\n",
      "count                  22831\n",
      "hate_speech            22831\n",
      "offensive_language     22831\n",
      "neither                22831\n",
      "class                  22831\n",
      "tweet                  22831\n",
      "classe                 22831\n",
      "tweet_novo             22831\n",
      "tweet_organizado       22831\n",
      "text_split             22831\n",
      "text_novo2             22831\n",
      "eric_tweet_definito    22831\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dados_eric.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "79ukgWlsmKrR"
   },
   "outputs": [],
   "source": [
    "##PEGANDO CADA TOKEN DOS TITULOS\n",
    "lista_lista_tokens = [titulo.split(\" \") for titulo in dados_eric['eric_tweet_definito'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "2u4vqX_Vo52R"
   },
   "outputs": [],
   "source": [
    "##PEGANDO CADA TOKEN DOS TITULOS HELANO\n",
    "lista_lista_tokens_helano = [titulo.split(\" \") for titulo in dados_helano['tweet_definitivo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njPEa3AvmSxN",
    "outputId": "ab6d133e-35c2-400a-ceb9-52b825ffea4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:24:37,643 - Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=300, alpha=0.03)', 'datetime': '2021-04-29T19:24:37.643404', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "2021-04-29 19:24:37,653 - collecting all words and their counts\n",
      "2021-04-29 19:24:37,654 - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-04-29 19:24:37,669 - PROGRESS: at sentence #5000, processed 46335 words, keeping 6129 word types\n",
      "2021-04-29 19:24:37,682 - PROGRESS: at sentence #10000, processed 90264 words, keeping 8522 word types\n",
      "2021-04-29 19:24:37,703 - PROGRESS: at sentence #15000, processed 140142 words, keeping 10364 word types\n",
      "2021-04-29 19:24:37,729 - PROGRESS: at sentence #20000, processed 192416 words, keeping 11902 word types\n",
      "2021-04-29 19:24:37,759 - collected 12923 word types from a corpus of 236957 raw words and 24783 sentences\n",
      "2021-04-29 19:24:37,760 - Creating a fresh vocabulary\n",
      "2021-04-29 19:24:37,781 - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4226 unique words (32.701385127292426%% of original 12923, drops 8697)', 'datetime': '2021-04-29T19:24:37.781358', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:24:37,783 - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 222108 word corpus (93.73346218934238%% of original 236957, drops 14849)', 'datetime': '2021-04-29T19:24:37.783187', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:24:37,816 - deleting the raw counts dictionary of 12923 items\n",
      "2021-04-29 19:24:37,817 - sample=0.001 downsamples 50 most-common words\n",
      "2021-04-29 19:24:37,821 - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 180318.33637486296 word corpus (81.2%% of prior 222108)', 'datetime': '2021-04-29T19:24:37.821022', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:24:37,881 - estimated required memory for 4226 words and 300 dimensions: 12255400 bytes\n",
      "2021-04-29 19:24:37,882 - resetting layer weights\n",
      "2021-04-29 19:24:37,897 - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-04-29T19:24:37.897451', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "## MONTANDO O VOCABULARIO com mensagem de LOG para acompanhar\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\",level=logging.INFO)\n",
    "\n",
    "w2v_modelo_cbow_helano = Word2Vec(sg = 0,window = 2,vector_size = 300,min_count = 5,alpha = 0.03,min_alpha = 0.07) ## ERA SIZE E MUDOU PARA VECTOR_SIZE\n",
    "\n",
    "## definindo através do progress_per de quantas em quantas interações o log será exibido.\n",
    "w2v_modelo_cbow_helano.build_vocab(lista_lista_tokens_helano,progress_per=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWpqF1F7xNHO",
    "outputId": "d6d88637-a1a6-4df7-aae9-3ab87c59443e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:25:19,714 - Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=300, alpha=0.03)', 'datetime': '2021-04-29T19:25:19.714557', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "2021-04-29 19:25:19,721 - collecting all words and their counts\n",
      "2021-04-29 19:25:19,722 - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-04-29 19:25:19,731 - PROGRESS: at sentence #5000, processed 34801 words, keeping 7955 word types\n",
      "2021-04-29 19:25:19,742 - PROGRESS: at sentence #10000, processed 70543 words, keeping 11847 word types\n",
      "2021-04-29 19:25:19,754 - PROGRESS: at sentence #15000, processed 109914 words, keeping 15191 word types\n",
      "2021-04-29 19:25:19,773 - PROGRESS: at sentence #20000, processed 148523 words, keeping 17747 word types\n",
      "2021-04-29 19:25:19,780 - collected 18996 word types from a corpus of 168936 raw words and 22831 sentences\n",
      "2021-04-29 19:25:19,781 - Creating a fresh vocabulary\n",
      "2021-04-29 19:25:19,809 - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 3945 unique words (20.76753000631712%% of original 18996, drops 15051)', 'datetime': '2021-04-29T19:25:19.809387', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:25:19,810 - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 145869 word corpus (86.34571672112516%% of original 168936, drops 23067)', 'datetime': '2021-04-29T19:25:19.810442', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:25:19,837 - deleting the raw counts dictionary of 18996 items\n",
      "2021-04-29 19:25:19,838 - sample=0.001 downsamples 40 most-common words\n",
      "2021-04-29 19:25:19,839 - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 116046.30357432053 word corpus (79.6%% of prior 145869)', 'datetime': '2021-04-29T19:25:19.839089', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:25:19,892 - estimated required memory for 3945 words and 300 dimensions: 11440500 bytes\n",
      "2021-04-29 19:25:19,893 - resetting layer weights\n",
      "2021-04-29 19:25:19,900 - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-04-29T19:25:19.900656', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "## MONTANDO O VOCABULARIO com mensagem de LOG para acompanhar\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\",level=logging.INFO)\n",
    "\n",
    "w2v_modelo_cbow_eric = Word2Vec(sg = 0,\n",
    "                      window = 2,\n",
    "                      vector_size = 300, ## ERA SIZE\n",
    "                      min_count = 5,\n",
    "                      alpha = 0.03,\n",
    "                      min_alpha = 0.07)\n",
    "\n",
    "## definindo através do progress_per de quantas em quantas interações o log será exibido.\n",
    "w2v_modelo_cbow_eric.build_vocab(lista_lista_tokens,progress_per=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj9Zuk7imeL4",
    "outputId": "7da20bad-ea2d-4dc2-f996-172d64c70640"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo_cbow_helano.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9Go-lsqay01",
    "outputId": "36689049-58b3-4134-ae88-1ed8491957dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22831"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo_cbow_eric.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "ML0W0fFYmlqN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:29:30,518 - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 3945 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2', 'datetime': '2021-04-29T19:29:30.518204', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "2021-04-29 19:29:30,726 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:30,731 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:30,734 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:30,735 - EPOCH - 1 : training on 168936 raw words (115978 effective words) took 0.2s, 623507 effective words/s\n",
      "2021-04-29 19:29:30,837 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:30,842 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:30,844 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:30,845 - EPOCH - 2 : training on 168936 raw words (115930 effective words) took 0.1s, 1156889 effective words/s\n",
      "2021-04-29 19:29:30,935 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:30,941 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:30,943 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:30,943 - EPOCH - 3 : training on 168936 raw words (115852 effective words) took 0.1s, 1288750 effective words/s\n",
      "2021-04-29 19:29:31,037 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,043 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,045 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,045 - EPOCH - 4 : training on 168936 raw words (115930 effective words) took 0.1s, 1416044 effective words/s\n",
      "2021-04-29 19:29:31,136 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,140 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,144 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,145 - EPOCH - 5 : training on 168936 raw words (116031 effective words) took 0.1s, 1415437 effective words/s\n",
      "2021-04-29 19:29:31,241 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,245 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,250 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,251 - EPOCH - 6 : training on 168936 raw words (116115 effective words) took 0.1s, 1176610 effective words/s\n",
      "2021-04-29 19:29:31,346 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,350 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,355 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,356 - EPOCH - 7 : training on 168936 raw words (116226 effective words) took 0.1s, 1186189 effective words/s\n",
      "2021-04-29 19:29:31,452 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,456 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,460 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,461 - EPOCH - 8 : training on 168936 raw words (116028 effective words) took 0.1s, 1271983 effective words/s\n",
      "2021-04-29 19:29:31,551 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,555 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,560 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,560 - EPOCH - 9 : training on 168936 raw words (115862 effective words) took 0.1s, 1387464 effective words/s\n",
      "2021-04-29 19:29:31,658 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,664 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,673 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,675 - EPOCH - 10 : training on 168936 raw words (116085 effective words) took 0.1s, 1173330 effective words/s\n",
      "2021-04-29 19:29:31,797 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,800 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,803 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,804 - EPOCH - 11 : training on 168936 raw words (116152 effective words) took 0.1s, 973116 effective words/s\n",
      "2021-04-29 19:29:31,904 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:31,912 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:31,915 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:31,916 - EPOCH - 12 : training on 168936 raw words (115885 effective words) took 0.1s, 1238678 effective words/s\n",
      "2021-04-29 19:29:32,026 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,031 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,032 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,033 - EPOCH - 13 : training on 168936 raw words (115941 effective words) took 0.1s, 1109768 effective words/s\n",
      "2021-04-29 19:29:32,140 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,145 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,147 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,148 - EPOCH - 14 : training on 168936 raw words (116111 effective words) took 0.1s, 1080489 effective words/s\n",
      "2021-04-29 19:29:32,258 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,265 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,267 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,268 - EPOCH - 15 : training on 168936 raw words (115924 effective words) took 0.1s, 1066801 effective words/s\n",
      "2021-04-29 19:29:32,386 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,389 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,392 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,393 - EPOCH - 16 : training on 168936 raw words (116094 effective words) took 0.1s, 993031 effective words/s\n",
      "2021-04-29 19:29:32,498 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,502 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,507 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,508 - EPOCH - 17 : training on 168936 raw words (115900 effective words) took 0.1s, 1080967 effective words/s\n",
      "2021-04-29 19:29:32,620 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,625 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,628 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,629 - EPOCH - 18 : training on 168936 raw words (116062 effective words) took 0.1s, 1094232 effective words/s\n",
      "2021-04-29 19:29:32,732 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,737 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,746 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,747 - EPOCH - 19 : training on 168936 raw words (116168 effective words) took 0.1s, 1057193 effective words/s\n",
      "2021-04-29 19:29:32,850 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,855 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,857 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,857 - EPOCH - 20 : training on 168936 raw words (116048 effective words) took 0.1s, 1123204 effective words/s\n",
      "2021-04-29 19:29:32,963 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:32,965 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:32,969 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:32,970 - EPOCH - 21 : training on 168936 raw words (116099 effective words) took 0.1s, 1159291 effective words/s\n",
      "2021-04-29 19:29:33,083 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,088 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,090 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,091 - EPOCH - 22 : training on 168936 raw words (116007 effective words) took 0.1s, 1018797 effective words/s\n",
      "2021-04-29 19:29:33,194 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,203 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,205 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,206 - EPOCH - 23 : training on 168936 raw words (115931 effective words) took 0.1s, 1105737 effective words/s\n",
      "2021-04-29 19:29:33,310 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,313 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,317 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,317 - EPOCH - 24 : training on 168936 raw words (115999 effective words) took 0.1s, 1111571 effective words/s\n",
      "2021-04-29 19:29:33,409 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,413 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,418 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,418 - EPOCH - 25 : training on 168936 raw words (116129 effective words) took 0.1s, 1261821 effective words/s\n",
      "2021-04-29 19:29:33,504 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,509 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,511 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,512 - EPOCH - 26 : training on 168936 raw words (116050 effective words) took 0.1s, 1422495 effective words/s\n",
      "2021-04-29 19:29:33,602 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,609 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,612 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,613 - EPOCH - 27 : training on 168936 raw words (116068 effective words) took 0.1s, 1244098 effective words/s\n",
      "2021-04-29 19:29:33,725 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,729 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,732 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,733 - EPOCH - 28 : training on 168936 raw words (115972 effective words) took 0.1s, 1080367 effective words/s\n",
      "2021-04-29 19:29:33,838 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,841 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,843 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,844 - EPOCH - 29 : training on 168936 raw words (116012 effective words) took 0.1s, 1226549 effective words/s\n",
      "2021-04-29 19:29:33,944 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:33,948 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:33,952 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:33,953 - EPOCH - 30 : training on 168936 raw words (115842 effective words) took 0.1s, 1173663 effective words/s\n",
      "2021-04-29 19:29:33,954 - Word2Vec lifecycle event {'msg': 'training on 5068080 raw words (3480431 effective words) took 3.4s, 1013485 effective words/s', 'datetime': '2021-04-29T19:29:33.954442', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3480431, 5068080)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "MkAy01jupQBB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:29:40,237 - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4226 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2', 'datetime': '2021-04-29T19:29:40.237421', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "2021-04-29 19:29:40,425 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:40,431 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:40,433 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:40,433 - EPOCH - 1 : training on 236957 raw words (180362 effective words) took 0.2s, 977914 effective words/s\n",
      "2021-04-29 19:29:40,575 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:40,582 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:40,583 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:40,583 - EPOCH - 2 : training on 236957 raw words (180228 effective words) took 0.1s, 1272950 effective words/s\n",
      "2021-04-29 19:29:40,724 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:40,728 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:40,730 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:40,730 - EPOCH - 3 : training on 236957 raw words (180476 effective words) took 0.1s, 1323933 effective words/s\n",
      "2021-04-29 19:29:40,866 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:40,871 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:40,872 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:40,873 - EPOCH - 4 : training on 236957 raw words (180193 effective words) took 0.1s, 1363386 effective words/s\n",
      "2021-04-29 19:29:41,009 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:41,014 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:41,015 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:41,016 - EPOCH - 5 : training on 236957 raw words (180333 effective words) took 0.1s, 1325927 effective words/s\n",
      "2021-04-29 19:29:41,155 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:41,161 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:41,162 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:41,163 - EPOCH - 6 : training on 236957 raw words (180426 effective words) took 0.1s, 1316043 effective words/s\n",
      "2021-04-29 19:29:41,304 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:41,307 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:41,308 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:41,309 - EPOCH - 7 : training on 236957 raw words (180348 effective words) took 0.1s, 1364192 effective words/s\n",
      "2021-04-29 19:29:41,468 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:41,476 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:41,477 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:41,477 - EPOCH - 8 : training on 236957 raw words (180355 effective words) took 0.2s, 1120465 effective words/s\n",
      "2021-04-29 19:29:41,627 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:41,633 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:41,634 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:41,635 - EPOCH - 9 : training on 236957 raw words (180580 effective words) took 0.1s, 1219035 effective words/s\n",
      "2021-04-29 19:29:41,790 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:41,795 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:41,797 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:41,798 - EPOCH - 10 : training on 236957 raw words (180072 effective words) took 0.2s, 1186183 effective words/s\n",
      "2021-04-29 19:29:41,954 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:41,957 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:41,961 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:41,962 - EPOCH - 11 : training on 236957 raw words (180391 effective words) took 0.1s, 1211879 effective words/s\n",
      "2021-04-29 19:29:42,120 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:42,121 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:42,122 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:42,123 - EPOCH - 12 : training on 236957 raw words (180332 effective words) took 0.1s, 1217394 effective words/s\n",
      "2021-04-29 19:29:42,282 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:42,289 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:42,290 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:42,291 - EPOCH - 13 : training on 236957 raw words (180110 effective words) took 0.2s, 1149416 effective words/s\n",
      "2021-04-29 19:29:42,445 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:42,453 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:42,453 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:42,454 - EPOCH - 14 : training on 236957 raw words (180348 effective words) took 0.2s, 1191117 effective words/s\n",
      "2021-04-29 19:29:42,609 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:42,613 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:42,618 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:42,618 - EPOCH - 15 : training on 236957 raw words (180331 effective words) took 0.2s, 1166053 effective words/s\n",
      "2021-04-29 19:29:42,794 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:42,799 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:42,801 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:42,801 - EPOCH - 16 : training on 236957 raw words (180223 effective words) took 0.2s, 1097888 effective words/s\n",
      "2021-04-29 19:29:42,966 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:42,971 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:42,975 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:42,976 - EPOCH - 17 : training on 236957 raw words (180093 effective words) took 0.2s, 1081106 effective words/s\n",
      "2021-04-29 19:29:43,138 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:43,146 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:43,147 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:43,147 - EPOCH - 18 : training on 236957 raw words (180354 effective words) took 0.2s, 1097115 effective words/s\n",
      "2021-04-29 19:29:43,310 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:43,314 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:43,317 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:43,318 - EPOCH - 19 : training on 236957 raw words (180487 effective words) took 0.2s, 1130819 effective words/s\n",
      "2021-04-29 19:29:43,460 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:43,464 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:43,468 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:43,468 - EPOCH - 20 : training on 236957 raw words (180166 effective words) took 0.1s, 1258950 effective words/s\n",
      "2021-04-29 19:29:43,599 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:43,604 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:43,605 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:43,606 - EPOCH - 21 : training on 236957 raw words (180366 effective words) took 0.1s, 1415522 effective words/s\n",
      "2021-04-29 19:29:43,748 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:43,751 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:43,753 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:43,753 - EPOCH - 22 : training on 236957 raw words (180637 effective words) took 0.1s, 1289133 effective words/s\n",
      "2021-04-29 19:29:43,895 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:43,900 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:43,901 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:43,902 - EPOCH - 23 : training on 236957 raw words (180230 effective words) took 0.1s, 1273983 effective words/s\n",
      "2021-04-29 19:29:44,051 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:44,057 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:44,059 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:44,059 - EPOCH - 24 : training on 236957 raw words (180178 effective words) took 0.1s, 1246231 effective words/s\n",
      "2021-04-29 19:29:44,207 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:44,212 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:44,213 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:44,214 - EPOCH - 25 : training on 236957 raw words (180124 effective words) took 0.1s, 1279040 effective words/s\n",
      "2021-04-29 19:29:44,356 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:44,360 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:44,366 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:44,367 - EPOCH - 26 : training on 236957 raw words (180279 effective words) took 0.1s, 1234421 effective words/s\n",
      "2021-04-29 19:29:44,516 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:44,522 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:44,524 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:44,525 - EPOCH - 27 : training on 236957 raw words (180507 effective words) took 0.1s, 1203788 effective words/s\n",
      "2021-04-29 19:29:44,676 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:44,682 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:44,686 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:44,687 - EPOCH - 28 : training on 236957 raw words (180231 effective words) took 0.2s, 1163828 effective words/s\n",
      "2021-04-29 19:29:44,848 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:44,854 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:44,855 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:44,856 - EPOCH - 29 : training on 236957 raw words (180233 effective words) took 0.2s, 1136046 effective words/s\n",
      "2021-04-29 19:29:45,007 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:29:45,010 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:29:45,011 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:29:45,012 - EPOCH - 30 : training on 236957 raw words (180168 effective words) took 0.1s, 1242506 effective words/s\n",
      "2021-04-29 19:29:45,013 - Word2Vec lifecycle event {'msg': 'training on 7108710 raw words (5409161 effective words) took 4.8s, 1132944 effective words/s', 'datetime': '2021-04-29T19:29:45.013314', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5409161, 7108710)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YsLEkcvmswi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "YKwlNLGBm21h"
   },
   "outputs": [],
   "source": [
    "# iniciando a chamada callback\n",
    "class callback(CallbackAny2Vec):\n",
    "  def __init__(self):\n",
    "    self.epoch = 0\n",
    "    \n",
    "  def on_epoch_end(self, model):\n",
    "       loss = model.get_latest_training_loss()\n",
    "       if self.epoch == 0:\n",
    "           print('Loss após a época {}: {}'.format(self.epoch, loss))\n",
    "       else:\n",
    "           print('Loss após a época {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "       self.epoch += 1\n",
    "       self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "bGkIKhbanf5i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:19,165 - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 3945 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2', 'datetime': '2021-04-29T19:30:19.165700', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "2021-04-29 19:30:19,295 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:19,301 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:19,305 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:19,306 - EPOCH - 1 : training on 168936 raw words (116063 effective words) took 0.1s, 939285 effective words/s\n",
      "2021-04-29 19:30:19,404 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:19,409 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:19,411 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:19,412 - EPOCH - 2 : training on 168936 raw words (115983 effective words) took 0.1s, 1180534 effective words/s\n",
      "2021-04-29 19:30:19,505 - worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 0: 53907.8359375\n",
      "Loss após a época 1: 53221.765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:19,509 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:19,511 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:19,512 - EPOCH - 3 : training on 168936 raw words (115851 effective words) took 0.1s, 1341685 effective words/s\n",
      "2021-04-29 19:30:19,597 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:19,601 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:19,604 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:19,604 - EPOCH - 4 : training on 168936 raw words (116009 effective words) took 0.1s, 1540089 effective words/s\n",
      "2021-04-29 19:30:19,688 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:19,692 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:19,694 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:19,695 - EPOCH - 5 : training on 168936 raw words (116070 effective words) took 0.1s, 1418560 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 2: 52756.6796875\n",
      "Loss após a época 3: 53156.53125\n",
      "Loss após a época 4: 52739.15625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:19,788 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:19,792 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:19,795 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:19,796 - EPOCH - 6 : training on 168936 raw words (116137 effective words) took 0.1s, 1340177 effective words/s\n",
      "2021-04-29 19:30:19,879 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:19,884 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:19,887 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:19,887 - EPOCH - 7 : training on 168936 raw words (116126 effective words) took 0.1s, 1598444 effective words/s\n",
      "2021-04-29 19:30:19,982 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:19,987 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:19,991 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:19,992 - EPOCH - 8 : training on 168936 raw words (115951 effective words) took 0.1s, 1199246 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 5: 52475.84375\n",
      "Loss após a época 6: 52072.78125\n",
      "Loss após a época 7: 52260.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:20,090 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,094 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,100 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,100 - EPOCH - 9 : training on 168936 raw words (115944 effective words) took 0.1s, 1246426 effective words/s\n",
      "2021-04-29 19:30:20,202 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,209 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,211 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,212 - EPOCH - 10 : training on 168936 raw words (116110 effective words) took 0.1s, 1225626 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 8: 51722.8125\n",
      "Loss após a época 9: 52697.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:20,316 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,323 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,325 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,325 - EPOCH - 11 : training on 168936 raw words (115896 effective words) took 0.1s, 1125944 effective words/s\n",
      "2021-04-29 19:30:20,425 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,430 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,435 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,436 - EPOCH - 12 : training on 168936 raw words (115979 effective words) took 0.1s, 1223643 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 10: 51328.0\n",
      "Loss após a época 11: 51299.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:20,535 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,538 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,543 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,543 - EPOCH - 13 : training on 168936 raw words (116023 effective words) took 0.1s, 1255787 effective words/s\n",
      "2021-04-29 19:30:20,650 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,653 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,657 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,658 - EPOCH - 14 : training on 168936 raw words (116126 effective words) took 0.1s, 1114310 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 12: 50799.75\n",
      "Loss após a época 13: 51374.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:20,751 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,755 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,757 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,757 - EPOCH - 15 : training on 168936 raw words (116182 effective words) took 0.1s, 1349381 effective words/s\n",
      "2021-04-29 19:30:20,846 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,850 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,853 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,854 - EPOCH - 16 : training on 168936 raw words (116000 effective words) took 0.1s, 1356600 effective words/s\n",
      "2021-04-29 19:30:20,941 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:20,945 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:20,948 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:20,949 - EPOCH - 17 : training on 168936 raw words (116042 effective words) took 0.1s, 1416544 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 14: 50023.875\n",
      "Loss após a época 15: 49734.625\n",
      "Loss após a época 16: 49821.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:21,037 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,044 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,046 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,046 - EPOCH - 18 : training on 168936 raw words (116080 effective words) took 0.1s, 1387092 effective words/s\n",
      "2021-04-29 19:30:21,139 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,143 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,145 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,145 - EPOCH - 19 : training on 168936 raw words (116138 effective words) took 0.1s, 1297973 effective words/s\n",
      "2021-04-29 19:30:21,233 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,238 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,240 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,240 - EPOCH - 20 : training on 168936 raw words (115965 effective words) took 0.1s, 1446479 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 17: 49045.0\n",
      "Loss após a época 18: 48195.125\n",
      "Loss após a época 19: 47987.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:21,333 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,337 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,340 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,341 - EPOCH - 21 : training on 168936 raw words (115946 effective words) took 0.1s, 1237486 effective words/s\n",
      "2021-04-29 19:30:21,429 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,433 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,436 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,437 - EPOCH - 22 : training on 168936 raw words (115990 effective words) took 0.1s, 1437781 effective words/s\n",
      "2021-04-29 19:30:21,526 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,530 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,533 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,533 - EPOCH - 23 : training on 168936 raw words (116172 effective words) took 0.1s, 1389143 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 20: 47992.3125\n",
      "Loss após a época 21: 46595.125\n",
      "Loss após a época 22: 46131.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:21,618 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,622 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,625 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,626 - EPOCH - 24 : training on 168936 raw words (115931 effective words) took 0.1s, 1517022 effective words/s\n",
      "2021-04-29 19:30:21,713 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,717 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,720 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,721 - EPOCH - 25 : training on 168936 raw words (115925 effective words) took 0.1s, 1459094 effective words/s\n",
      "2021-04-29 19:30:21,802 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,806 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,809 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,810 - EPOCH - 26 : training on 168936 raw words (116127 effective words) took 0.1s, 1512158 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 23: 44862.625\n",
      "Loss após a época 24: 42989.75\n",
      "Loss após a época 25: 42904.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:21,891 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,896 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,898 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,898 - EPOCH - 27 : training on 168936 raw words (115958 effective words) took 0.1s, 1418122 effective words/s\n",
      "2021-04-29 19:30:21,976 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:21,980 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:21,982 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:21,983 - EPOCH - 28 : training on 168936 raw words (116036 effective words) took 0.1s, 1663416 effective words/s\n",
      "2021-04-29 19:30:22,067 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:22,070 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:22,072 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:22,073 - EPOCH - 29 : training on 168936 raw words (116219 effective words) took 0.1s, 1462439 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 26: 41476.625\n",
      "Loss após a época 27: 39736.125\n",
      "Loss após a época 28: 39428.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:30:22,152 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:30:22,156 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:30:22,158 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:30:22,158 - EPOCH - 30 : training on 168936 raw words (115966 effective words) took 0.1s, 1554033 effective words/s\n",
      "2021-04-29 19:30:22,159 - Word2Vec lifecycle event {'msg': 'training on 5068080 raw words (3480945 effective words) took 3.0s, 1163385 effective words/s', 'datetime': '2021-04-29T19:30:22.159598', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 29: 37272.125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3480945, 5068080)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo_cbow_eric.train(lista_lista_tokens,\n",
    "                total_examples=w2v_modelo_cbow_eric.corpus_count,\n",
    "                epochs = 30,\n",
    "                compute_loss = True,\n",
    "                callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prize', 0.3201570510864258),\n",
       " ('jets', 0.31379517912864685),\n",
       " ('holds', 0.30752310156822205),\n",
       " ('horsey', 0.30421167612075806),\n",
       " ('season', 0.3026547431945801),\n",
       " ('treading', 0.29793864488601685),\n",
       " ('points', 0.29273083806037903),\n",
       " ('patted', 0.2840335965156555),\n",
       " ('picking', 0.27932408452033997),\n",
       " ('woof', 0.2775574326515198)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:24,326 - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4226 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2', 'datetime': '2021-04-29T19:31:24.325976', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "2021-04-29 19:31:24,481 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:24,484 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:24,485 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:24,486 - EPOCH - 1 : training on 236957 raw words (180343 effective words) took 0.1s, 1205724 effective words/s\n",
      "2021-04-29 19:31:24,623 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:24,626 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:24,627 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:24,627 - EPOCH - 2 : training on 236957 raw words (180478 effective words) took 0.1s, 1414838 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 0: 97542.6015625\n",
      "Loss após a época 1: 97456.5703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:24,765 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:24,773 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:24,774 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:24,775 - EPOCH - 3 : training on 236957 raw words (180600 effective words) took 0.1s, 1316764 effective words/s\n",
      "2021-04-29 19:31:24,904 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:24,907 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:24,908 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:24,909 - EPOCH - 4 : training on 236957 raw words (180247 effective words) took 0.1s, 1460645 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 2: 97203.828125\n",
      "Loss após a época 3: 97340.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:25,038 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:25,042 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:25,043 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:25,044 - EPOCH - 5 : training on 236957 raw words (180226 effective words) took 0.1s, 1480685 effective words/s\n",
      "2021-04-29 19:31:25,169 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:25,172 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:25,173 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:25,174 - EPOCH - 6 : training on 236957 raw words (180357 effective words) took 0.1s, 1473106 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 4: 92699.125\n",
      "Loss após a época 5: 92530.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:25,303 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:25,309 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:25,311 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:25,311 - EPOCH - 7 : training on 236957 raw words (180288 effective words) took 0.1s, 1379953 effective words/s\n",
      "2021-04-29 19:31:25,432 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:25,436 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:25,437 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:25,437 - EPOCH - 8 : training on 236957 raw words (180316 effective words) took 0.1s, 1561937 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 6: 91773.875\n",
      "Loss após a época 7: 90689.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:25,567 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:25,573 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:25,574 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:25,575 - EPOCH - 9 : training on 236957 raw words (180325 effective words) took 0.1s, 1412284 effective words/s\n",
      "2021-04-29 19:31:25,699 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:25,705 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:25,707 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:25,707 - EPOCH - 10 : training on 236957 raw words (180350 effective words) took 0.1s, 1520081 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 8: 90735.625\n",
      "Loss após a época 9: 88315.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:25,846 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:25,852 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:25,854 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:25,855 - EPOCH - 11 : training on 236957 raw words (180109 effective words) took 0.1s, 1318307 effective words/s\n",
      "2021-04-29 19:31:26,001 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:26,004 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:26,005 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:26,006 - EPOCH - 12 : training on 236957 raw words (180404 effective words) took 0.1s, 1273176 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 10: 86214.8125\n",
      "Loss após a época 11: 88534.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:26,147 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:26,151 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:26,154 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:26,155 - EPOCH - 13 : training on 236957 raw words (180484 effective words) took 0.1s, 1292800 effective words/s\n",
      "2021-04-29 19:31:26,292 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:26,295 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:26,298 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:26,299 - EPOCH - 14 : training on 236957 raw words (180323 effective words) took 0.1s, 1412663 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 12: 80991.125\n",
      "Loss após a época 13: 78137.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:26,429 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:26,436 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:26,437 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:26,437 - EPOCH - 15 : training on 236957 raw words (180352 effective words) took 0.1s, 1439771 effective words/s\n",
      "2021-04-29 19:31:26,571 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:26,575 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:26,576 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:26,576 - EPOCH - 16 : training on 236957 raw words (180125 effective words) took 0.1s, 1395738 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 14: 74862.125\n",
      "Loss após a época 15: 69773.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:26,709 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:26,716 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:26,719 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:26,720 - EPOCH - 17 : training on 236957 raw words (180665 effective words) took 0.1s, 1405371 effective words/s\n",
      "2021-04-29 19:31:26,854 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:26,860 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:26,860 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:26,861 - EPOCH - 18 : training on 236957 raw words (180229 effective words) took 0.1s, 1358403 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 16: 66346.375\n",
      "Loss após a época 17: 64560.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:26,997 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:27,001 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:27,002 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:27,003 - EPOCH - 19 : training on 236957 raw words (180372 effective words) took 0.1s, 1380533 effective words/s\n",
      "2021-04-29 19:31:27,127 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:27,130 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:27,132 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:27,133 - EPOCH - 20 : training on 236957 raw words (180203 effective words) took 0.1s, 1483294 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 18: 57569.75\n",
      "Loss após a época 19: 53944.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:27,252 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:27,255 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:27,257 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:27,258 - EPOCH - 21 : training on 236957 raw words (180602 effective words) took 0.1s, 1566062 effective words/s\n",
      "2021-04-29 19:31:27,386 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:27,391 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:27,392 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:27,393 - EPOCH - 22 : training on 236957 raw words (180373 effective words) took 0.1s, 1413169 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 20: 51408.375\n",
      "Loss após a época 21: 48130.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:27,528 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:27,533 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:27,533 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:27,534 - EPOCH - 23 : training on 236957 raw words (180134 effective words) took 0.1s, 1375104 effective words/s\n",
      "2021-04-29 19:31:27,663 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:27,669 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:27,670 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:27,671 - EPOCH - 24 : training on 236957 raw words (180051 effective words) took 0.1s, 1415368 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 22: 45319.75\n",
      "Loss após a época 23: 43828.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:27,800 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:27,806 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:27,807 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:27,808 - EPOCH - 25 : training on 236957 raw words (180344 effective words) took 0.1s, 1463371 effective words/s\n",
      "2021-04-29 19:31:27,931 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:27,935 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:27,936 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:27,937 - EPOCH - 26 : training on 236957 raw words (180372 effective words) took 0.1s, 1583098 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 24: 39944.5\n",
      "Loss após a época 25: 38037.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:28,068 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:28,071 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:28,073 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:28,074 - EPOCH - 27 : training on 236957 raw words (180381 effective words) took 0.1s, 1412527 effective words/s\n",
      "2021-04-29 19:31:28,201 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:28,204 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:28,205 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:28,206 - EPOCH - 28 : training on 236957 raw words (180119 effective words) took 0.1s, 1448551 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 26: 35997.625\n",
      "Loss após a época 27: 34853.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:31:28,322 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:28,326 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:28,327 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:28,328 - EPOCH - 29 : training on 236957 raw words (180231 effective words) took 0.1s, 1604143 effective words/s\n",
      "2021-04-29 19:31:28,458 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:31:28,460 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:31:28,463 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:31:28,465 - EPOCH - 30 : training on 236957 raw words (180403 effective words) took 0.1s, 1434133 effective words/s\n",
      "2021-04-29 19:31:28,466 - Word2Vec lifecycle event {'msg': 'training on 7108710 raw words (5409806 effective words) took 4.1s, 1307030 effective words/s', 'datetime': '2021-04-29T19:31:28.465999', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 28: 32868.25\n",
      "Loss após a época 29: 31902.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5409806, 7108710)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo_cbow_helano.train(lista_lista_tokens_helano,\n",
    "                total_examples=w2v_modelo_cbow_helano.corpus_count,\n",
    "                epochs = 30,\n",
    "                compute_loss = True,\n",
    "                callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "EmAf4vA8nj-g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:49,689 - Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=300, alpha=0.03)', 'datetime': '2021-04-29T19:32:49.689217', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "2021-04-29 19:32:49,690 - collecting all words and their counts\n",
      "2021-04-29 19:32:49,691 - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-04-29 19:32:49,701 - PROGRESS: at sentence #5000, processed 46335 words, keeping 6129 word types\n",
      "2021-04-29 19:32:49,711 - PROGRESS: at sentence #10000, processed 90264 words, keeping 8522 word types\n",
      "2021-04-29 19:32:49,721 - PROGRESS: at sentence #15000, processed 140142 words, keeping 10364 word types\n",
      "2021-04-29 19:32:49,733 - PROGRESS: at sentence #20000, processed 192416 words, keeping 11902 word types\n",
      "2021-04-29 19:32:49,751 - collected 12923 word types from a corpus of 236957 raw words and 24783 sentences\n",
      "2021-04-29 19:32:49,752 - Creating a fresh vocabulary\n",
      "2021-04-29 19:32:49,780 - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4226 unique words (32.701385127292426%% of original 12923, drops 8697)', 'datetime': '2021-04-29T19:32:49.780935', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:32:49,781 - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 222108 word corpus (93.73346218934238%% of original 236957, drops 14849)', 'datetime': '2021-04-29T19:32:49.781781', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:32:49,820 - deleting the raw counts dictionary of 12923 items\n",
      "2021-04-29 19:32:49,821 - sample=0.001 downsamples 50 most-common words\n",
      "2021-04-29 19:32:49,822 - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 180318.33637486296 word corpus (81.2%% of prior 222108)', 'datetime': '2021-04-29T19:32:49.822097', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:32:49,882 - estimated required memory for 4226 words and 300 dimensions: 12255400 bytes\n",
      "2021-04-29 19:32:49,883 - resetting layer weights\n",
      "2021-04-29 19:32:49,891 - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-04-29T19:32:49.891714', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2021-04-29 19:32:49,892 - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4226 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-04-29T19:32:49.892875', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "2021-04-29 19:32:50,272 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:50,276 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:50,279 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:50,280 - EPOCH - 1 : training on 236957 raw words (180417 effective words) took 0.4s, 474578 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 0: 688765.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:50,683 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:50,684 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:50,691 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:50,691 - EPOCH - 2 : training on 236957 raw words (180346 effective words) took 0.4s, 450992 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 1: 640172.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:51,096 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:51,097 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:51,107 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:51,107 - EPOCH - 3 : training on 236957 raw words (180275 effective words) took 0.4s, 443734 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 2: 638995.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:51,519 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:51,523 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:51,532 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:51,532 - EPOCH - 4 : training on 236957 raw words (180173 effective words) took 0.4s, 433470 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 3: 609139.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:51,956 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:51,964 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:51,966 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:51,967 - EPOCH - 5 : training on 236957 raw words (180272 effective words) took 0.4s, 426035 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 4: 592156.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:52,385 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:52,386 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:52,387 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:52,388 - EPOCH - 6 : training on 236957 raw words (180243 effective words) took 0.4s, 436558 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 5: 584795.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:52,797 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:52,799 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:52,807 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:52,808 - EPOCH - 7 : training on 236957 raw words (180382 effective words) took 0.4s, 440143 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 6: 570021.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:53,228 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:53,236 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:53,241 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:53,242 - EPOCH - 8 : training on 236957 raw words (180304 effective words) took 0.4s, 424845 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 7: 529154.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:53,671 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:53,672 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:53,681 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:53,682 - EPOCH - 9 : training on 236957 raw words (180332 effective words) took 0.4s, 418283 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 8: 536406.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:54,172 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:54,176 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:54,178 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:54,178 - EPOCH - 10 : training on 236957 raw words (180289 effective words) took 0.5s, 370020 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 9: 534772.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:54,627 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:54,628 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:54,634 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:54,635 - EPOCH - 11 : training on 236957 raw words (180213 effective words) took 0.4s, 400697 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 10: 536736.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:55,060 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:55,062 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:55,064 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:55,065 - EPOCH - 12 : training on 236957 raw words (180098 effective words) took 0.4s, 432913 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 11: 533714.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:55,472 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:55,475 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:55,482 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:55,483 - EPOCH - 13 : training on 236957 raw words (180203 effective words) took 0.4s, 445283 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 12: 538548.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:55,875 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:55,880 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:55,888 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:55,889 - EPOCH - 14 : training on 236957 raw words (180121 effective words) took 0.4s, 456454 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 13: 531148.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:56,292 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:56,293 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:56,300 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:56,301 - EPOCH - 15 : training on 236957 raw words (180321 effective words) took 0.4s, 448249 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 14: 515766.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:56,728 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:56,731 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:56,737 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:56,738 - EPOCH - 16 : training on 236957 raw words (180158 effective words) took 0.4s, 422179 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 15: 497035.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:57,143 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:57,147 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:57,149 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:57,150 - EPOCH - 17 : training on 236957 raw words (180361 effective words) took 0.4s, 452919 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 16: 502221.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:57,534 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:57,538 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:57,544 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:57,544 - EPOCH - 18 : training on 236957 raw words (180445 effective words) took 0.4s, 468870 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 17: 497320.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:57,925 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:57,928 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:57,930 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:57,930 - EPOCH - 19 : training on 236957 raw words (180295 effective words) took 0.4s, 483548 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 18: 494817.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:58,319 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:58,320 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:58,328 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:58,329 - EPOCH - 20 : training on 236957 raw words (180358 effective words) took 0.4s, 463816 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 19: 496196.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:58,739 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:58,744 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:58,749 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:58,750 - EPOCH - 21 : training on 236957 raw words (180233 effective words) took 0.4s, 438157 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 20: 501922.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:59,175 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:59,180 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:59,184 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:59,185 - EPOCH - 22 : training on 236957 raw words (180345 effective words) took 0.4s, 425738 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 21: 497442.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:59,583 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:59,588 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:59,591 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:59,592 - EPOCH - 23 : training on 236957 raw words (180421 effective words) took 0.4s, 453493 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 22: 502739.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:32:59,978 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:32:59,979 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:32:59,984 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:32:59,984 - EPOCH - 24 : training on 236957 raw words (180160 effective words) took 0.4s, 471143 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 23: 497662.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:33:00,368 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:33:00,372 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:33:00,375 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:33:00,376 - EPOCH - 25 : training on 236957 raw words (180561 effective words) took 0.4s, 473650 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 24: 495552.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:33:00,761 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:33:00,762 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:33:00,771 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:33:00,772 - EPOCH - 26 : training on 236957 raw words (180442 effective words) took 0.4s, 468422 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 25: 507658.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:33:01,192 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:33:01,194 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:33:01,205 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:33:01,206 - EPOCH - 27 : training on 236957 raw words (180436 effective words) took 0.4s, 426324 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 26: 499497.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:33:01,615 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:33:01,617 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:33:01,625 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:33:01,626 - EPOCH - 28 : training on 236957 raw words (180358 effective words) took 0.4s, 440909 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 27: 502264.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:33:02,039 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:33:02,040 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:33:02,046 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:33:02,047 - EPOCH - 29 : training on 236957 raw words (180317 effective words) took 0.4s, 438624 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 28: 499747.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:33:02,471 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:33:02,474 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:33:02,484 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:33:02,485 - EPOCH - 30 : training on 236957 raw words (180328 effective words) took 0.4s, 419654 effective words/s\n",
      "2021-04-29 19:33:02,486 - Word2Vec lifecycle event {'msg': 'training on 7108710 raw words (5409207 effective words) took 12.6s, 429558 effective words/s', 'datetime': '2021-04-29T19:33:02.486121', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 29: 499961.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5409207, 7108710)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TREINANDO O MODELO COM SKIPGRAM HELANO\n",
    "\n",
    "w2v_modelo_sg_helano = Word2Vec(sg = 1,\n",
    "                      window = 5,\n",
    "                      vector_size = 300,\n",
    "                      min_count = 5,\n",
    "                      alpha = 0.03,\n",
    "                      min_alpha = 0.07)\n",
    "\n",
    "## definindo através do progress_per de quantas em quantas interações o log será exibido.\n",
    "w2v_modelo_sg_helano.build_vocab(lista_lista_tokens_helano,progress_per=5000)\n",
    "\n",
    "\n",
    "### TREINAMENTO\n",
    "\n",
    "w2v_modelo_sg_helano.train(lista_lista_tokens_helano,\n",
    "                total_examples=w2v_modelo_sg_helano.corpus_count,\n",
    "                epochs = 30,\n",
    "                compute_loss = True,\n",
    "                callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "NJ1nXhnGxWxM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:54,397 - Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=300, alpha=0.03)', 'datetime': '2021-04-29T19:34:54.397531', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'created'}\n",
      "2021-04-29 19:34:54,399 - collecting all words and their counts\n",
      "2021-04-29 19:34:54,400 - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-04-29 19:34:54,408 - PROGRESS: at sentence #5000, processed 34801 words, keeping 7955 word types\n",
      "2021-04-29 19:34:54,416 - PROGRESS: at sentence #10000, processed 70543 words, keeping 11847 word types\n",
      "2021-04-29 19:34:54,425 - PROGRESS: at sentence #15000, processed 109914 words, keeping 15191 word types\n",
      "2021-04-29 19:34:54,433 - PROGRESS: at sentence #20000, processed 148523 words, keeping 17747 word types\n",
      "2021-04-29 19:34:54,438 - collected 18996 word types from a corpus of 168936 raw words and 22831 sentences\n",
      "2021-04-29 19:34:54,439 - Creating a fresh vocabulary\n",
      "2021-04-29 19:34:54,460 - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 3945 unique words (20.76753000631712%% of original 18996, drops 15051)', 'datetime': '2021-04-29T19:34:54.460811', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:34:54,461 - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 145869 word corpus (86.34571672112516%% of original 168936, drops 23067)', 'datetime': '2021-04-29T19:34:54.461654', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:34:54,490 - deleting the raw counts dictionary of 18996 items\n",
      "2021-04-29 19:34:54,491 - sample=0.001 downsamples 40 most-common words\n",
      "2021-04-29 19:34:54,494 - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 116046.30357432053 word corpus (79.6%% of prior 145869)', 'datetime': '2021-04-29T19:34:54.494258', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-04-29 19:34:54,543 - estimated required memory for 3945 words and 300 dimensions: 11440500 bytes\n",
      "2021-04-29 19:34:54,544 - resetting layer weights\n",
      "2021-04-29 19:34:54,556 - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-04-29T19:34:54.556160', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2021-04-29 19:34:54,557 - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 3945 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-04-29T19:34:54.557022', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "2021-04-29 19:34:54,765 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:54,789 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:54,791 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:54,791 - EPOCH - 1 : training on 168936 raw words (115924 effective words) took 0.2s, 529048 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 0: 399550.46875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:54,996 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:55,014 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:55,021 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:55,022 - EPOCH - 2 : training on 168936 raw words (116178 effective words) took 0.2s, 529479 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 1: 362717.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:55,241 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:55,267 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:55,270 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:55,271 - EPOCH - 3 : training on 168936 raw words (116092 effective words) took 0.2s, 488628 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 2: 354526.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:55,489 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:55,511 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:55,514 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:55,515 - EPOCH - 4 : training on 168936 raw words (116059 effective words) took 0.2s, 508413 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 3: 342561.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:55,739 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:55,760 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:55,769 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:55,770 - EPOCH - 5 : training on 168936 raw words (116043 effective words) took 0.2s, 481252 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 4: 330368.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:56,027 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:56,046 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:56,053 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:56,054 - EPOCH - 6 : training on 168936 raw words (116054 effective words) took 0.3s, 419246 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 5: 316429.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:56,268 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:56,290 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:56,295 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:56,296 - EPOCH - 7 : training on 168936 raw words (116060 effective words) took 0.2s, 514653 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 6: 298778.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:56,506 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:56,530 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:56,533 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:56,534 - EPOCH - 8 : training on 168936 raw words (116024 effective words) took 0.2s, 535253 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 7: 297413.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:56,761 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:56,792 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:56,793 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:56,794 - EPOCH - 9 : training on 168936 raw words (116059 effective words) took 0.2s, 473143 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 8: 289776.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:57,035 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:57,063 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:57,064 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:57,065 - EPOCH - 10 : training on 168936 raw words (115922 effective words) took 0.3s, 440724 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 9: 286013.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:57,299 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:57,325 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:57,331 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:57,331 - EPOCH - 11 : training on 168936 raw words (116068 effective words) took 0.3s, 448509 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 10: 276515.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:57,552 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:57,569 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:57,578 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:57,578 - EPOCH - 12 : training on 168936 raw words (116016 effective words) took 0.2s, 485810 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 11: 275467.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:57,785 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:57,806 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:57,810 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:57,811 - EPOCH - 13 : training on 168936 raw words (115979 effective words) took 0.2s, 524676 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 12: 273383.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:58,015 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:58,034 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:58,041 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:58,041 - EPOCH - 14 : training on 168936 raw words (115932 effective words) took 0.2s, 543712 effective words/s\n",
      "2021-04-29 19:34:58,242 - worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 13: 263632.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:58,262 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:58,268 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:58,269 - EPOCH - 15 : training on 168936 raw words (116078 effective words) took 0.2s, 536253 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 14: 258333.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:58,500 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:58,515 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:58,531 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:58,531 - EPOCH - 16 : training on 168936 raw words (116035 effective words) took 0.3s, 458531 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 15: 257300.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:58,759 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:58,772 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:58,787 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:58,788 - EPOCH - 17 : training on 168936 raw words (116041 effective words) took 0.2s, 465812 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 16: 259162.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:59,009 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:59,029 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:59,031 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:59,032 - EPOCH - 18 : training on 168936 raw words (116113 effective words) took 0.2s, 493478 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 17: 259335.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:59,244 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:59,270 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:59,271 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:59,272 - EPOCH - 19 : training on 168936 raw words (116103 effective words) took 0.2s, 522337 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 18: 265751.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:59,474 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:59,497 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:59,500 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:59,500 - EPOCH - 20 : training on 168936 raw words (116210 effective words) took 0.2s, 536038 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 19: 260739.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:59,712 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:34:59,737 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:34:59,741 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:34:59,742 - EPOCH - 21 : training on 168936 raw words (116056 effective words) took 0.2s, 501817 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 20: 260616.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:34:59,978 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:00,001 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:00,007 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:00,008 - EPOCH - 22 : training on 168936 raw words (115988 effective words) took 0.3s, 452665 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 21: 259770.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:35:00,246 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:00,266 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:00,273 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:00,274 - EPOCH - 23 : training on 168936 raw words (116008 effective words) took 0.2s, 464837 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 22: 260186.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:35:00,502 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:00,526 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:00,527 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:00,528 - EPOCH - 24 : training on 168936 raw words (116018 effective words) took 0.2s, 477899 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 23: 259840.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:35:00,769 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:00,796 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:00,797 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:00,797 - EPOCH - 25 : training on 168936 raw words (115980 effective words) took 0.3s, 448047 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 24: 268162.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:35:01,032 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:01,056 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:01,062 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:01,063 - EPOCH - 26 : training on 168936 raw words (116043 effective words) took 0.3s, 454444 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 25: 261247.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:35:01,287 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:01,301 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:01,310 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:01,311 - EPOCH - 27 : training on 168936 raw words (116146 effective words) took 0.2s, 489463 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 26: 261351.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:35:01,526 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:01,547 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:01,552 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:01,553 - EPOCH - 28 : training on 168936 raw words (116077 effective words) took 0.2s, 496044 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 27: 263345.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:35:01,794 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:01,823 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:01,824 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:01,825 - EPOCH - 29 : training on 168936 raw words (116143 effective words) took 0.3s, 447645 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 28: 262661.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:35:02,050 - worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-29 19:35:02,077 - worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-29 19:35:02,079 - worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-29 19:35:02,080 - EPOCH - 30 : training on 168936 raw words (116282 effective words) took 0.2s, 468151 effective words/s\n",
      "2021-04-29 19:35:02,081 - Word2Vec lifecycle event {'msg': 'training on 5068080 raw words (3481731 effective words) took 7.5s, 462781 effective words/s', 'datetime': '2021-04-29T19:35:02.081229', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 29: 258017.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3481731, 5068080)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TREINANDO O MODELO COM SKIPGRAM\n",
    "\n",
    "w2v_modelo_sg_eric = Word2Vec(sg = 1,\n",
    "                      window = 5,\n",
    "                      vector_size = 300,\n",
    "                      min_count = 5,\n",
    "                      alpha = 0.03,\n",
    "                      min_alpha = 0.07)\n",
    "\n",
    "## definindo através do progress_per de quantas em quantas interações o log será exibido.\n",
    "w2v_modelo_sg_eric.build_vocab(lista_lista_tokens,progress_per=5000)\n",
    "\n",
    "\n",
    "### TREINAMENTO\n",
    "\n",
    "w2v_modelo_sg_eric.train(lista_lista_tokens,\n",
    "                total_examples=w2v_modelo_sg_eric.corpus_count,\n",
    "                epochs = 30,\n",
    "                compute_loss = True,\n",
    "                callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXAGT417nue6",
    "outputId": "570bd009-733a-47a1-e7af-04efa523f77c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:36:14,065 - storing 4226x300 projection weights into /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow_helano.txt\n",
      "2021-04-29 19:36:14,873 - storing 4226x300 projection weights into /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram_helano.txt\n"
     ]
    }
   ],
   "source": [
    "w2v_modelo_cbow_helano.wv.save_word2vec_format('/Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow_helano.txt',binary=False)\n",
    "w2v_modelo_sg_helano.wv.save_word2vec_format('/Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram_helano.txt',binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gI3B_bpFxf61",
    "outputId": "960d1fa7-57fc-4720-9bc1-8ca5727228aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-29 19:37:02,696 - storing 3945x300 projection weights into /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow.txt\n",
      "2021-04-29 19:37:03,414 - storing 3945x300 projection weights into /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram.txt\n"
     ]
    }
   ],
   "source": [
    "w2v_modelo_cbow_eric.wv.save_word2vec_format('/Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow.txt',binary=False)\n",
    "w2v_modelo_sg_eric.wv.save_word2vec_format('/Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram.txt',binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12TNXPhDoWZi"
   },
   "source": [
    "###AGORA FAZER REGRESSAO LOGISTICA - BASTA IMPORTAR OS TXT e o CSV COM OS CAMPOS ADICIONADOS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "peHTBt8zhuaY",
    "outputId": "d2195f07-754f-4ecb-d1b3-7341eb80e9d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "      <th>tweet_novo</th>\n",
       "      <th>tweet_organizado</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_novo2</th>\n",
       "      <th>tweet_definitivo</th>\n",
       "      <th>eric_tweet_definito</th>\n",
       "      <th>helano_tweet_definito</th>\n",
       "      <th>eric_tweet_definito_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>['Neutro']</td>\n",
       "      <td>rt    as a woman you shouldn't complain abou...</td>\n",
       "      <td>womann't complain cleaning house amp man alway...</td>\n",
       "      <td>['woman', 'n', \"'\", 't', 'complain', 'cleaning...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>rt woman complain cleaning house amp man trash</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>rt woman complain cleaning house amp man trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>['Linguagem ofensiva']</td>\n",
       "      <td>rt    boy dats cold tyga dwn bad for cuffin ...</td>\n",
       "      <td>boy dats cold tyga dwn bad cuffin dat hoe st p...</td>\n",
       "      <td>['boy', 'dat', 's', 'cold', 'tyga', 'dw', 'n',...</td>\n",
       "      <td>boy dat s cold tyga dw n bad cuff in dat hoe s...</td>\n",
       "      <td>boy dat s cold tea do n bad cuff in dat he st ...</td>\n",
       "      <td>rt boy dats cold tyga dwn bad cuffin dat hoe p...</td>\n",
       "      <td>boy dat s cold tea do n bad cuff in dat he st ...</td>\n",
       "      <td>rt boy dats cold tyga dwn bad cuffin dat hoe p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>['Linguagem ofensiva']</td>\n",
       "      <td>rt   dawg  rt    you ever fuck a bitch and s...</td>\n",
       "      <td>dawg ever fuck bitch start cry confused shit</td>\n",
       "      <td>['daw', 'g', 'ever', 'fuck', 'bitch', 'start',...</td>\n",
       "      <td>daw g ever fuck bitch start cry confused shit</td>\n",
       "      <td>day g ever fuck bitch start cry confused shit</td>\n",
       "      <td>rt dawg rt fuck bitch start cry confused shit</td>\n",
       "      <td>day g ever fuck bitch start cry confused shit</td>\n",
       "      <td>rt dawg rt fuck bitch start cry confused shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>['Linguagem ofensiva']</td>\n",
       "      <td>rt  _g_anderson   _based she look like a tranny</td>\n",
       "      <td>_g_anderson _based look like tranny</td>\n",
       "      <td>['g', 'anderson', 'based', 'look', 'like', 'tr...</td>\n",
       "      <td>g anderson based look like tr an ny</td>\n",
       "      <td>g anderson based look like tr an ny</td>\n",
       "      <td>rt look like tranny</td>\n",
       "      <td>g anderson based look like tr an ny</td>\n",
       "      <td>rt look like tranny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>['Linguagem ofensiva']</td>\n",
       "      <td>rt    the shit you hear about me might be tr...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "      <td>['shit', 'hear', 'might', 'true', 'might', 'fa...</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "      <td>shit hear might true might baker bitch told a</td>\n",
       "      <td>rt shit hear true faker bitch told ya</td>\n",
       "      <td>shit hear might true might baker bitch told a</td>\n",
       "      <td>rt shit hear true faker bitch told ya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  count  hate_speech  \\\n",
       "0           0             0               0      3            0   \n",
       "1           1             1               1      3            0   \n",
       "2           2             2               2      3            0   \n",
       "3           3             3               3      3            0   \n",
       "4           4             4               4      6            0   \n",
       "\n",
       "   offensive_language  neither  class  \\\n",
       "0                   0        3      2   \n",
       "1                   3        0      1   \n",
       "2                   3        0      1   \n",
       "3                   2        1      1   \n",
       "4                   6        0      1   \n",
       "\n",
       "                                               tweet                  classe  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...              ['Neutro']   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  ['Linguagem ofensiva']   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  ['Linguagem ofensiva']   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  ['Linguagem ofensiva']   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  ['Linguagem ofensiva']   \n",
       "\n",
       "                                          tweet_novo  \\\n",
       "0    rt    as a woman you shouldn't complain abou...   \n",
       "1    rt    boy dats cold tyga dwn bad for cuffin ...   \n",
       "2    rt   dawg  rt    you ever fuck a bitch and s...   \n",
       "3    rt  _g_anderson   _based she look like a tranny   \n",
       "4    rt    the shit you hear about me might be tr...   \n",
       "\n",
       "                                    tweet_organizado  \\\n",
       "0  womann't complain cleaning house amp man alway...   \n",
       "1  boy dats cold tyga dwn bad cuffin dat hoe st p...   \n",
       "2       dawg ever fuck bitch start cry confused shit   \n",
       "3                _g_anderson _based look like tranny   \n",
       "4     shit hear might true might faker bitch told ya   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  ['woman', 'n', \"'\", 't', 'complain', 'cleaning...   \n",
       "1  ['boy', 'dat', 's', 'cold', 'tyga', 'dw', 'n',...   \n",
       "2  ['daw', 'g', 'ever', 'fuck', 'bitch', 'start',...   \n",
       "3  ['g', 'anderson', 'based', 'look', 'like', 'tr...   \n",
       "4  ['shit', 'hear', 'might', 'true', 'might', 'fa...   \n",
       "\n",
       "                                          text_novo2  \\\n",
       "0  woman n' t complain cleaning house amp man alw...   \n",
       "1  boy dat s cold tyga dw n bad cuff in dat hoe s...   \n",
       "2      daw g ever fuck bitch start cry confused shit   \n",
       "3                g anderson based look like tr an ny   \n",
       "4     shit hear might true might faker bitch told ya   \n",
       "\n",
       "                                    tweet_definitivo  \\\n",
       "0  woman n' t complain cleaning house amp man alw...   \n",
       "1  boy dat s cold tea do n bad cuff in dat he st ...   \n",
       "2      day g ever fuck bitch start cry confused shit   \n",
       "3                g anderson based look like tr an ny   \n",
       "4      shit hear might true might baker bitch told a   \n",
       "\n",
       "                                 eric_tweet_definito  \\\n",
       "0     rt woman complain cleaning house amp man trash   \n",
       "1  rt boy dats cold tyga dwn bad cuffin dat hoe p...   \n",
       "2      rt dawg rt fuck bitch start cry confused shit   \n",
       "3                                rt look like tranny   \n",
       "4              rt shit hear true faker bitch told ya   \n",
       "\n",
       "                               helano_tweet_definito  \\\n",
       "0  woman n' t complain cleaning house amp man alw...   \n",
       "1  boy dat s cold tea do n bad cuff in dat he st ...   \n",
       "2      day g ever fuck bitch start cry confused shit   \n",
       "3                g anderson based look like tr an ny   \n",
       "4      shit hear might true might baker bitch told a   \n",
       "\n",
       "                           eric_tweet_definito_final  \n",
       "0     rt woman complain cleaning house amp man trash  \n",
       "1  rt boy dats cold tyga dwn bad cuffin dat hoe p...  \n",
       "2      rt dawg rt fuck bitch start cry confused shit  \n",
       "3                                rt look like tranny  \n",
       "4              rt shit hear true faker bitch told ya  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Dataset\n",
    "#dados = pd.read_csv('/content/drive/MyDrive/Projeto Daniel/procesado_labeled_data_new.csv')\n",
    "#dados = pd.read_csv('/Users/eric/Downloads/projeto_daniel/procesado_labeled_data_new.csv')\n",
    "#dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HXn-ZX4un-U1"
   },
   "outputs": [],
   "source": [
    "#w2v_modelo_cbow_eric = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Projeto Daniel/w2v_modelo_cbow.txt\")\n",
    "#w2v_modelo_eric_sg = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Projeto Daniel/w2v_modelo_skipgram.txt\")\n",
    "#w2v_modelo_cbow_helano = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Projeto Daniel/w2v_modelo_cbow_helano.txt\")\n",
    "#w2v_modelo_sg_helano = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Projeto Daniel/w2v_modelo_skipgram_helano.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-30 09:29:56,126 - loading projection weights from /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow.txt\n",
      "2021-04-30 09:29:57,240 - KeyedVectors lifecycle event {'msg': 'loaded (3945, 300) matrix of type float32 from /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-04-30T09:29:57.236006', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n",
      "2021-04-30 09:29:57,242 - loading projection weights from /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram.txt\n",
      "2021-04-30 09:29:58,326 - KeyedVectors lifecycle event {'msg': 'loaded (3945, 300) matrix of type float32 from /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-04-30T09:29:58.326270', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n",
      "2021-04-30 09:29:58,327 - loading projection weights from /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow_helano.txt\n",
      "2021-04-30 09:29:59,404 - KeyedVectors lifecycle event {'msg': 'loaded (4226, 300) matrix of type float32 from /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow_helano.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-04-30T09:29:59.404542', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n",
      "2021-04-30 09:29:59,405 - loading projection weights from /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram_helano.txt\n",
      "2021-04-30 09:30:00,649 - KeyedVectors lifecycle event {'msg': 'loaded (4226, 300) matrix of type float32 from /Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram_helano.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-04-30T09:30:00.649590', 'gensim': '4.0.1', 'python': '3.7.4 (default, Aug 13 2019, 15:17:50) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.3.0-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "w2v_modelo_cbow_eric = KeyedVectors.load_word2vec_format(\"/Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow.txt\")\n",
    "w2v_modelo_sg_eric = KeyedVectors.load_word2vec_format(\"/Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram.txt\")\n",
    "w2v_modelo_cbow_helano = KeyedVectors.load_word2vec_format(\"/Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_cbow_helano.txt\")\n",
    "w2v_modelo_sg_helano = KeyedVectors.load_word2vec_format(\"/Users/eric/Downloads/projeto_daniel/29042021w2v_modelo_skipgram_helano.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "ee3S0oDtWW1Y"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm',disable=['paser','ner','tagger','textcat', \"lemmatizer\"])\n",
    "#nlp = en_core_sci_lg.load(disable=[\"tagger\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "def tokenizador(texto):\n",
    "  tokens_validos=[]\n",
    "  doc=nlp(str(texto))\n",
    "  for token in doc:\n",
    "    e_valido= not token.is_stop and token.is_alpha\n",
    "    if e_valido:\n",
    "      tokens_validos.append(token.text.lower())\n",
    "  \n",
    "  return tokens_validos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "aU3ZLoMEWmbY"
   },
   "outputs": [],
   "source": [
    "def combinacao_vetores_por_soma(palavras,modelo):\n",
    "  vetor_resultante = np.zeros((1,300))\n",
    "  for pn in palavras:\n",
    "    try:\n",
    "      vetor_resultante += modelo.get_vector(pn)\n",
    "    except KeyError:\n",
    "      pass\n",
    "  return vetor_resultante \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "c-FfhCJCYVWb"
   },
   "outputs": [],
   "source": [
    "def matriz_vetores(textos,modelo):\n",
    "  x=len(textos)\n",
    "  y=300\n",
    "  matriz = np.zeros((x,y))\n",
    "\n",
    "  for i in range(x):\n",
    "    palavras = tokenizador(textos.iloc[i])\n",
    "    matriz[i] = combinacao_vetores_por_soma(palavras,modelo)\n",
    "\n",
    "  return matriz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "RCCxia4rk202",
    "outputId": "f59345cf-b60c-40c2-df43-ab00e4802ad2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "      <th>tweet_novo</th>\n",
       "      <th>tweet_organizado</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_novo2</th>\n",
       "      <th>eric_tweet_definito</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[Neutro]</td>\n",
       "      <td>rt    as a woman you shouldn't complain abou...</td>\n",
       "      <td>womann't complain cleaning house amp man alway...</td>\n",
       "      <td>[woman, n, ', t, complain, cleaning, house, am...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>rt woman complain cleaning house amp man trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    boy dats cold tyga dwn bad for cuffin ...</td>\n",
       "      <td>boy dats cold tyga dwn bad cuffin dat hoe st p...</td>\n",
       "      <td>[boy, dat, s, cold, tyga, dw, n, bad, cuff, in...</td>\n",
       "      <td>boy dat s cold tyga dw n bad cuff in dat hoe s...</td>\n",
       "      <td>rt boy dats cold tyga dwn bad cuffin dat hoe p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet                classe  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...              [Neutro]   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  [Linguagem ofensiva]   \n",
       "\n",
       "                                          tweet_novo  \\\n",
       "0    rt    as a woman you shouldn't complain abou...   \n",
       "1    rt    boy dats cold tyga dwn bad for cuffin ...   \n",
       "\n",
       "                                    tweet_organizado  \\\n",
       "0  womann't complain cleaning house amp man alway...   \n",
       "1  boy dats cold tyga dwn bad cuffin dat hoe st p...   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [woman, n, ', t, complain, cleaning, house, am...   \n",
       "1  [boy, dat, s, cold, tyga, dw, n, bad, cuff, in...   \n",
       "\n",
       "                                          text_novo2  \\\n",
       "0  woman n' t complain cleaning house amp man alw...   \n",
       "1  boy dat s cold tyga dw n bad cuff in dat hoe s...   \n",
       "\n",
       "                                 eric_tweet_definito  \n",
       "0     rt woman complain cleaning house amp man trash  \n",
       "1  rt boy dats cold tyga dwn bad cuffin dat hoe p...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>classe</th>\n",
       "      <th>tweet_novo</th>\n",
       "      <th>tweet_organizado</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_novo2</th>\n",
       "      <th>tweet_definitivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[Neutro]</td>\n",
       "      <td>rt    as a woman you shouldn't complain abou...</td>\n",
       "      <td>womann't complain cleaning house amp man alway...</td>\n",
       "      <td>[woman, n, ', t, complain, cleaning, house, am...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "      <td>woman n' t complain cleaning house amp man alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[Linguagem ofensiva]</td>\n",
       "      <td>rt    boy dats cold tyga dwn bad for cuffin ...</td>\n",
       "      <td>boy dats cold tyga dwn bad cuffin dat hoe st p...</td>\n",
       "      <td>[boy, dat, s, cold, tyga, dw, n, bad, cuff, in...</td>\n",
       "      <td>boy dat s cold tyga dw n bad cuff in dat hoe s...</td>\n",
       "      <td>boy dat s cold tea do n bad cuff in dat he st ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet                classe  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...              [Neutro]   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  [Linguagem ofensiva]   \n",
       "\n",
       "                                          tweet_novo  \\\n",
       "0    rt    as a woman you shouldn't complain abou...   \n",
       "1    rt    boy dats cold tyga dwn bad for cuffin ...   \n",
       "\n",
       "                                    tweet_organizado  \\\n",
       "0  womann't complain cleaning house amp man alway...   \n",
       "1  boy dats cold tyga dwn bad cuffin dat hoe st p...   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [woman, n, ', t, complain, cleaning, house, am...   \n",
       "1  [boy, dat, s, cold, tyga, dw, n, bad, cuff, in...   \n",
       "\n",
       "                                          text_novo2  \\\n",
       "0  woman n' t complain cleaning house amp man alw...   \n",
       "1  boy dat s cold tyga dw n bad cuff in dat hoe s...   \n",
       "\n",
       "                                    tweet_definitivo  \n",
       "0  woman n' t complain cleaning house amp man alw...  \n",
       "1  boy dat s cold tea do n bad cuff in dat he st ...  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(dados_eric.head(2))\n",
    "dados_helano.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "Qk6-p1lZifrX"
   },
   "outputs": [],
   "source": [
    "### DEFININDO DADOS DE TREINO E TESTES \n",
    "X_helano = dados_helano['tweet_definitivo']\n",
    "\n",
    "X_eric = dados_eric['eric_tweet_definito']\n",
    "y_helano = dados_helano['class']\n",
    "y_eric = dados_eric['class']\n",
    "### TESTE\n",
    "#y_eric=MultiLabelBinarizer().fit_transform(y_eric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "24778    1\n",
       "24779    2\n",
       "24780    1\n",
       "24781    1\n",
       "24782    2\n",
       "Name: class, Length: 22831, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "BqDqqqpzi6jQ"
   },
   "outputs": [],
   "source": [
    "## TESTE COM 20% percentual que helano utilizou.\n",
    "X_train_helano, X_test_helano, y_train_helano, y_test_helano = train_test_split(X_helano, y_helano, random_state=42, test_size=0.2)\n",
    "#X_train_eric, X_test_eric, y_train_eric, y_test_eric = train_test_split(X_eric, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OznwFjl1jKZ8",
    "outputId": "6721c727-3497-4f70-c52a-f360ccfff677"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"woman n' t complain cleaning house amp man always take trash\""
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_helano[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eric, X_test_eric, y_train_eric, y_test_eric = train_test_split(X_eric, y_eric, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18878    1\n",
       "19780    2\n",
       "5780     1\n",
       "3781     2\n",
       "12790    1\n",
       "        ..\n",
       "13424    0\n",
       "23408    1\n",
       "6296     1\n",
       "907      1\n",
       "17326    1\n",
       "Name: class, Length: 18264, dtype: int64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2qQTuW8j4NR"
   },
   "source": [
    "##CBOW ERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "LrhMZYoTYjQ6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18264, 300)\n",
      "(4567, 300)\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "matriz_vetores_treino_cbow_eric = matriz_vetores(X_train_eric,w2v_modelo_cbow_eric)\n",
    "matriz_vetores_teste_cbow_eric = matriz_vetores(X_test_eric, w2v_modelo_cbow_eric)\n",
    "print(matriz_vetores_treino_cbow_eric.shape)\n",
    "print(matriz_vetores_teste_cbow_eric.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826, 300)\n",
      "(4957, 300)\n"
     ]
    }
   ],
   "source": [
    "matriz_vetores_treino_cbow_helano = matriz_vetores(X_train_helano,w2v_modelo_cbow_helano)\n",
    "matriz_vetores_teste_cbow_helano = matriz_vetores(X_test_helano, w2v_modelo_cbow_helano)\n",
    "print(matriz_vetores_treino_cbow_helano.shape)\n",
    "print(matriz_vetores_teste_cbow_helano.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrEghsFxOTlu"
   },
   "source": [
    "##SKIPGRAM HELANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "iog4eKrAOWeI",
    "outputId": "54d28256-4777-4b7d-b2fa-da2585ba0df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18264, 300)\n",
      "(4567, 300)\n"
     ]
    }
   ],
   "source": [
    "matriz_vetores_treino_skip_eric = matriz_vetores(X_train_eric,w2v_modelo_sg_eric)\n",
    "matriz_vetores_teste_skip_eric = matriz_vetores(X_test_eric, w2v_modelo_sg_eric)\n",
    "print(matriz_vetores_treino_skip_eric.shape)\n",
    "print(matriz_vetores_teste_skip_eric.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "LtxG0yhbO1wn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826, 300)\n",
      "(4957, 300)\n"
     ]
    }
   ],
   "source": [
    "matriz_vetores_treino_skip_helano = matriz_vetores(X_train_helano,w2v_modelo_sg_helano)\n",
    "matriz_vetores_teste_skip_helano = matriz_vetores(X_test_helano, w2v_modelo_sg_helano)\n",
    "print(matriz_vetores_treino_skip_helano.shape)\n",
    "print(matriz_vetores_teste_skip_helano.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REGRESSAO LOGISTICA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classificador(modelo,x_treino,y_treino,x_teste,y_teste):\n",
    "    LR = LogisticRegression(max_iter=800,solver='lbfgs', multi_class='auto',random_state=42)\n",
    "\n",
    "    LR.fit(x_treino,y_treino)\n",
    "\n",
    "\n",
    "  #calculando o y_pred\n",
    "    previsao = LR.predict(x_teste)\n",
    "    target_names = ['Discurso de ódio', 'Linguagem ofensiva', 'Neutro']\n",
    "\n",
    "\n",
    "    resultado = classification_report(y_teste,previsao,target_names=target_names)\n",
    "\n",
    "    print(resultado)\n",
    "\n",
    "\n",
    "\n",
    "    return LR,previsao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "  Discurso de ódio       0.47      0.15      0.22       280\n",
      "Linguagem ofensiva       0.89      0.95      0.92      3514\n",
      "            Neutro       0.76      0.72      0.74       773\n",
      "\n",
      "          accuracy                           0.86      4567\n",
      "         macro avg       0.71      0.60      0.63      4567\n",
      "      weighted avg       0.84      0.86      0.85      4567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RL__CBOW_eric = classificador(w2v_modelo_cbow_eric,matriz_vetores_treino_cbow_eric,\n",
    "              y_train_eric,\n",
    "              matriz_vetores_teste_cbow_eric,\n",
    "              y_test_eric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "  Discurso de ódio       0.46      0.16      0.23       290\n",
      "Linguagem ofensiva       0.89      0.95      0.92      3832\n",
      "            Neutro       0.75      0.70      0.72       835\n",
      "\n",
      "          accuracy                           0.86      4957\n",
      "         macro avg       0.70      0.60      0.62      4957\n",
      "      weighted avg       0.84      0.86      0.85      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RL__CBOW_helano = classificador(w2v_modelo_cbow_helano,matriz_vetores_treino_cbow_helano,\n",
    "              y_train_helano,\n",
    "              matriz_vetores_teste_cbow_helano,\n",
    "              y_test_helano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "  Discurso de ódio       0.50      0.15      0.23       280\n",
      "Linguagem ofensiva       0.90      0.95      0.92      3514\n",
      "            Neutro       0.76      0.74      0.75       773\n",
      "\n",
      "          accuracy                           0.87      4567\n",
      "         macro avg       0.72      0.61      0.64      4567\n",
      "      weighted avg       0.85      0.87      0.85      4567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RL__SKIP_eric = classificador(w2v_modelo_sg_eric,matriz_vetores_treino_skip_eric,\n",
    "              y_train_eric,\n",
    "              matriz_vetores_teste_skip_eric,\n",
    "              y_test_eric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "  Discurso de ódio       0.42      0.10      0.17       290\n",
      "Linguagem ofensiva       0.88      0.95      0.91      3832\n",
      "            Neutro       0.73      0.66      0.69       835\n",
      "\n",
      "          accuracy                           0.85      4957\n",
      "         macro avg       0.68      0.57      0.59      4957\n",
      "      weighted avg       0.83      0.85      0.83      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RL__SKIP_helano = classificador(w2v_modelo_sg_helano,matriz_vetores_treino_skip_helano,\n",
    "              y_train_helano,\n",
    "              matriz_vetores_teste_skip_helano,\n",
    "              y_test_helano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classificador_Twitter_sem_Tensor_Flow_tratamento_inicial_helano.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
